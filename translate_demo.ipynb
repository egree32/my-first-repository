{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [py35]",
      "language": "python",
      "name": "Python [py35]"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/egree32/my-first-repository/blob/master/translate_demo.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "X35e8L02Dz6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Translation project: use of Google Cloud Speech API Client Library for Python\n",
        "In this notebook, we demonstrate how to transcribe an audio recording. \n",
        "Prior to using the Speech API it is recommended that you read \n",
        "https://cloud.google.com/speech/docs/basics\n",
        "\n",
        "\n",
        "It begins with\n",
        "> This document is a guide to the basics of using the Google Cloud Speech API. This conceptual guide covers the types of requests you can make to the Speech API, how to construct those requests, and how to handle their responses. \n",
        "\n",
        "The Speech API has three main methods to perform speech recognition: \n",
        "- **Asynchronous Recognition** initiates a Long Running Operation. Using this operation, you can periodically poll for recognition results. Use asynchronous requests for audio data of any duration up to 180 minutes.\n",
        "- **Synchronous Recognition** Synchronous recognition requests are limited to audio data of 1 minute or less in duration.\n",
        "- **Streaming Recognition** Streaming requests are designed for real-time recognition purposes, such as capturing live audio from a microphone. Streaming recognition provides interim results while audio is being captured, allowing result to appear, for example, while a user is still speaking."
      ]
    },
    {
      "metadata": {
        "id": "vikHF1IIDz6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Flac files to cloud storage"
      ]
    },
    {
      "metadata": {
        "id": "Vb1Vw5btDz6L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The recording we will transcribe was saved in a `.flac` file. FLAC stands for Free Lossless Audio Codec, and as the name indicates, it is an open source, lossless, audio codec. In the file, I selected a segment from 11:16 to 11:36 as it was among the cleanest segments with respect to background noise. In this segment only a single voice (francophone) is heard. The total length of the original recording file was one hour and 8 minutes.\n",
        "\n",
        "Note: Google Speech API only supports mono recording. Also, 16000 Hertz is optimal. "
      ]
    },
    {
      "metadata": {
        "id": "CS8b-JJdDz6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing the data "
      ]
    },
    {
      "metadata": {
        "id": "yeXMzrU2Dz6N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To see the list of all your buckets, use the cell magic below."
      ]
    },
    {
      "metadata": {
        "id": "LsAcr1omDz6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0ab441c-80ea-4ed0-a371-cdf39d850515"
      },
      "cell_type": "code",
      "source": [
        " %%gcs list"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%gcs` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DhW65CMRDz6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another way to see the bucket name is to create a reference, below called `project`, to the project name. We will use this in the code that follows so that you can run this notebook in a new instance of datalab without having to retype names. \n",
        "The Datalab APIs are provided in the `google.datalab` Python library."
      ]
    },
    {
      "metadata": {
        "id": "TFrEski1Dz6W",
        "colab_type": "code",
        "colab": {},
        "outputId": "62c30933-ca9d-4f73-c993-5a6f4de99584"
      },
      "cell_type": "code",
      "source": [
        "from google.datalab import Context\n",
        "project_name = Context.default().project_id\n",
        "print(project_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
              "          <script>\n",
              "            requirejs.config({\n",
              "              paths: {\n",
              "                base: '/static/base',\n",
              "              },\n",
              "            });\n",
              "          </script>\n",
              "          "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "testing-data-lab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-LoLpvqpDz6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Cloud Storage functionality is contained within the `google.datalab.storage` module. Using the bucket name, we create a reference to the bucket. To see what the bucket contains, we enumerate through the objects in `project_bucket` to display the contents of the bucket. "
      ]
    },
    {
      "metadata": {
        "id": "zVCVVc1jDz6c",
        "colab_type": "code",
        "colab": {},
        "outputId": "e667e1ab-2b97-4090-e30c-4bb92fc8fe78"
      },
      "cell_type": "code",
      "source": [
        "import google.datalab.storage as storage\n",
        "project_bucket = storage.Bucket(project_name)\n",
        "for obj in project_bucket.objects():\n",
        "  print(obj.key)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
              "          <script>\n",
              "            requirejs.config({\n",
              "              paths: {\n",
              "                base: '/static/base',\n",
              "              },\n",
              "            });\n",
              "          </script>\n",
              "          "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "datalab-backups/us-central1-a/avaya/content/daily-20170815184107\n",
            "datalab-backups/us-central1-a/avaya/content/daily-20170818191007\n",
            "datalab-backups/us-central1-a/avaya/content/daily-20170823163046\n",
            "datalab-backups/us-central1-a/avaya/content/daily-20170901221818\n",
            "datalab-backups/us-central1-a/avaya/content/hourly-20170815184107\n",
            "datalab-backups/us-central1-a/avaya/content/hourly-20170818191007\n",
            "datalab-backups/us-central1-a/avaya/content/hourly-20170823163046\n",
            "datalab-backups/us-central1-a/avaya/content/hourly-20170901221818\n",
            "datalab-backups/us-central1-a/avaya/content/weekly-20170815184107\n",
            "datalab-backups/us-central1-a/avaya/content/weekly-20170818191007\n",
            "datalab-backups/us-central1-a/avaya/content/weekly-20170823163046\n",
            "datalab-backups/us-central1-a/avaya/content/weekly-20170901221818\n",
            "datalab-backups/us-central1-a/cloud/content/daily-20170824134323\n",
            "datalab-backups/us-central1-a/cloud/content/hourly-20170824134323\n",
            "datalab-backups/us-central1-a/cloud/content/weekly-20170824134323\n",
            "original_flac_data/\n",
            "original_flac_data/Arkoun1.flac\n",
            "original_flac_data/arkoun_selection_01116_01126.flac\n",
            "original_flac_data/arkoun_selection_01116_01126_mono.flac\n",
            "original_flac_data/demo_clip.flac\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdXM3bGwDz6h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can use the project name to construct a path to the flac file uploaded using Storage in the Cloud Console."
      ]
    },
    {
      "metadata": {
        "id": "cCW8CvQ0Dz6i",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7e09ec2-f61b-4e92-9934-0f9ed0045970"
      },
      "cell_type": "code",
      "source": [
        "demo_flac_file = 'gs://' + project_name + '/original_flac_data/demo_clip.flac'\n",
        "print('Flac file: ' + demo_flac_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
              "          <script>\n",
              "            requirejs.config({\n",
              "              paths: {\n",
              "                base: '/static/base',\n",
              "              },\n",
              "            });\n",
              "          </script>\n",
              "          "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Flac file: gs://testing-data-lab/original_flac_data/demo_clip.flac\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fp-gM-5-Dz6m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transcription demonstration\n",
        "The code in the next cell is from the Google Cloud Speech API Python Samples \n",
        "\n",
        "https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/speech/cloud-client/transcribe.py"
      ]
    },
    {
      "metadata": {
        "id": "sYX-3ZyRDz6o",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a50f1df-3efc-4685-f877-151ffd07e047"
      },
      "cell_type": "code",
      "source": [
        "# [START def_transcribe_gcs]\n",
        "def transcribe_gcs(gcs_uri):\n",
        "    \"\"\"Transcribes the audio file specified by the gcs_uri.\"\"\"\n",
        "    from google.cloud import speech\n",
        "    from google.cloud.speech import enums\n",
        "    from google.cloud.speech import types\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    # [START migration_audio_config_gcs]\n",
        "    audio = types.RecognitionAudio(uri=gcs_uri)\n",
        "    config = types.RecognitionConfig(\n",
        "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
        "        sample_rate_hertz=16000,\n",
        "        language_code='fr-FR')\n",
        "    # [END migration_audio_config_gcs]\n",
        "\n",
        "    response = client.recognize(config, audio)\n",
        "    # Print the first alternative of all the consecutive results.\n",
        "    for result in response.results:\n",
        "        print('Transcript: {}'.format(result.alternatives[0].transcript))\n",
        "# [END def_transcribe_gcs]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
              "          <script>\n",
              "            requirejs.config({\n",
              "              paths: {\n",
              "                base: '/static/base',\n",
              "              },\n",
              "            });\n",
              "          </script>\n",
              "          "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "A88kzOmwDz6r",
        "colab_type": "code",
        "colab": {},
        "outputId": "bf455f1c-783c-407a-dc10-039d13da0c76"
      },
      "cell_type": "code",
      "source": [
        "transcribe_gcs(demo_flac_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
              "          <script>\n",
              "            requirejs.config({\n",
              "              paths: {\n",
              "                base: '/static/base',\n",
              "              },\n",
              "            });\n",
              "          </script>\n",
              "          "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Transcript: observer cette réalité est le devenir de cette réalité pour tirer chaque fois des leçons vue de définir une politique d'enseignement une politique d'intervention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NXTv2FiSDz6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The demo file was 15 seconds of a french speaker in a car. Here is the audio clip that was transcribed above\n",
        "\n",
        "\n",
        "https://storage.googleapis.com/testing-data-lab/original_flac_data/demo_clip.flac"
      ]
    }
  ]
}